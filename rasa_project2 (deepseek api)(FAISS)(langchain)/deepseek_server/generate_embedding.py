import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import os
import pickle

# Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n file
CSV_PATH = "../database/database.csv"
QUESTION_EMBEDDINGS_PATH = "./embeddings/question_vectors.npy"
ANSWER_EMBEDDINGS_PATH = "./embeddings/answer_vectors.npy"
QUESTION_INDEX_PATH = "./embeddings/index_question.faiss"
ANSWER_INDEX_PATH = "./embeddings/index_answer.faiss"
BM25_INDEX_PATH = "./embeddings/bm25.pkl"
# Load dataset
print("ğŸ”„ Äang táº£i database...")
df = pd.read_csv(CSV_PATH, encoding="utf-8")[['Question', 'Answer']]
df = df.apply(lambda x: x.str.lower().str.strip())

# Load Sentence Transformer model
print("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh embedding...")
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# HÃ m chuáº©n hÃ³a vectors
def normalize(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / (norms + 1e-10)

# Táº¡o embeddings
print("ğŸ”„ Äang táº¡o embedding cho cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i...")
question_vectors = normalize(embedding_model.encode(df["Question"].tolist()).astype('float32'))
answer_vectors = normalize(embedding_model.encode(df["Answer"].tolist()).astype('float32'))

# LÆ°u embeddings vÃ o file
print("ğŸ’¾ Äang lÆ°u embeddings vÃ o file...")
np.save(QUESTION_EMBEDDINGS_PATH, question_vectors)
np.save(ANSWER_EMBEDDINGS_PATH, answer_vectors)

# Táº¡o FAISS index
print("ğŸ”„ Äang táº¡o FAISS index...")
index_question = faiss.IndexHNSWFlat(question_vectors.shape[1], 32)
index_question.add(question_vectors)
index_answer = faiss.IndexHNSWFlat(answer_vectors.shape[1], 32)
index_answer.add(answer_vectors)

# LÆ°u FAISS index
print("ğŸ’¾ Äang lÆ°u FAISS index...")
faiss.write_index(index_question, QUESTION_INDEX_PATH)
faiss.write_index(index_answer, ANSWER_INDEX_PATH)

# ğŸ”¹ Táº¡o BM25 corpus riÃªng biá»‡t cho cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i
tokenized_questions = [q.split() for q in df['Question'].tolist()]
tokenized_answers = [a.split() for a in df['Answer'].tolist()]

# ğŸ”¹ Táº¡o BM25 index riÃªng biá»‡t
bm25_questions = BM25Okapi(tokenized_questions)
bm25_answers = BM25Okapi(tokenized_answers)

# ğŸ”¹ LÆ°u BM25 index
with open(BM25_INDEX_PATH, "wb") as f:
    pickle.dump((bm25_questions, bm25_answers, df.to_dict(orient='records')), f)

print("âœ… Embeddings, FAISS index vÃ  BM25 Ä‘Ã£ Ä‘Æ°á»£c táº¡o & lÆ°u!")
