import os
import json
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# Load API key t·ª´ file .env
load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

# Kh·ªüi t·∫°o ·ª©ng d·ª•ng FastAPI
app = FastAPI()

# Ki·ªÉm tra API key
if not DEEPSEEK_API_KEY:
    raise ValueError("üö® L·ªói: Ch∆∞a c√≥ API key. H√£y ƒë·∫∑t DEEPSEEK_API_KEY trong file .env!")

# Kh·ªüi t·∫°o m√¥ h√¨nh DeepSeek
llm = ChatOpenAI(
    openai_api_key=DEEPSEEK_API_KEY,
    openai_api_base="https://openrouter.ai/api/v1",
    model="deepseek/deepseek-chat:free",
    temperature=0.5,
    max_tokens=1024,
)

# ƒê·ªãnh nghƒ©a d·ªØ li·ªáu ƒë·∫ßu v√†o
class EvalRequest(BaseModel):
    question: str
    contexts: list
    answer: str

@app.post("/evaluate")
def evaluate_response(request: EvalRequest):
    """
    API ƒë√°nh gi√° chatbot d·ª±a tr√™n DeepSeek:
    - ƒê·ªô ch√≠nh x√°c (Faithfulness)
    - M·ª©c ƒë·ªô li√™n quan (Context Precision)
    - ƒê·ªô ƒë√∫ng c·ªßa c√¢u tr·∫£ l·ªùi (Answer Correctness)
    """
    prompt = f"""
        You are an expert in evaluating chatbots. Please assess the response based on the following criteria:  
        1Ô∏è‚É£ **Faithfulness**: Does the answer rely on the information provided in the contexts?  
        2Ô∏è‚É£ **Context Precision**: Does the answer align with the question?  
        3Ô∏è‚É£ **Answer Correctness**: Is the answer correct?  

        Data:  
        - **Question**: {request.question}  
        - **Contexts**: {request.contexts}  
        - **Answer**: {request.answer}  

        **No explanation is needed**, just provide ratings from 0 to 1 (0: lowest, 1: highest).  
        Return the evaluation results in the following JSON format:  
        {{  
            "faithfulness": provide a decimal number without rounding from 0 to 1 (e.g., 0.99233),  
            "context_precision": 0-1,  
            "answer_correctness": 0-1  
        }}

    """

    # G·ª≠i request ƒë·∫øn DeepSeek
    response = llm.invoke(prompt)

    #debug
    print("Question: ",request.question)
    print("Contexts: ",request.contexts)
    print("Answer: ",request.answer)

    # Ki·ªÉm tra response c√≥ ·ªü d·∫°ng JSON kh√¥ng
    try:
        json_str = response.content.strip("```json\n").strip("\n```")  # Lo·∫°i b·ªè markdown JSON
        json_data = json.loads(json_str)  # Chuy·ªÉn v·ªÅ Python dict
    except Exception as e:
        print("üö® L·ªói x·ª≠ l√Ω JSON:", e)
        json_data = {"error": "Ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh kh√¥ng ph·∫£i JSON h·ª£p l·ªá.", "raw_response": response.content}

    print("üìä K·∫øt qu·∫£ ƒë√°nh gi√° JSON:", json_data)  # Debug

    return json_data  # Tr·∫£ v·ªÅ JSON chu·∫©n

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
