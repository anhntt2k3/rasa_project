import chromadb
import pickle
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ğŸ“‚ Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n
CHROMA_DB_PATH = "./chroma_db"
BM25_INDEX_PATH = "./embeddings/bm25.pkl"

# ğŸ› ï¸ Load mÃ´ hÃ¬nh embedding
print("ğŸ”„ Äang táº£i mÃ´ hÃ¬nh embedding...")
bi_encoder = SentenceTransformer("intfloat/e5-base-v2")
cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2")

# ğŸ› ï¸ Káº¿t ná»‘i ChromaDB
print("ğŸ”„ Äang káº¿t ná»‘i ChromaDB...")
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
collection = chroma_client.get_or_create_collection(name="qa_collection")

# ğŸ› ï¸ Load BM25 index tá»« file
print("ğŸ”„ Äang táº£i BM25 index...")
with open(BM25_INDEX_PATH, "rb") as f:
    bm25_combined, doc_map = pickle.load(f)

# HÃ m remove_semantic_duplicates
def remove_semantic_duplicates(results, threshold=0.9):
    """Loáº¡i bá» cÃ¡c cÃ¢u há»i trÃ¹ng láº·p vá» Ã½ nghÄ©a báº±ng cosine similarity"""
    if not results:
        return []

    questions = [q for q, _ in results]
    embeddings = bi_encoder.encode(questions)  # MÃ£ hÃ³a táº¥t cáº£ cÃ¢u há»i thÃ nh vector
    similarity_matrix = cosine_similarity(embeddings)

    unique_results = []
    seen_indices = set()

    for i in range(len(results)):
        if i in seen_indices:
            continue
        unique_results.append(results[i])
        
        # ÄÃ¡nh dáº¥u nhá»¯ng cÃ¢u cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao Ä‘á»ƒ bá» qua
        for j in range(i + 1, len(results)):
            if similarity_matrix[i, j] > threshold:
                seen_indices.add(j)

    return unique_results

# ğŸ” TÃ¬m kiáº¿m trong ChromaDB
def search_chromadb(query, top_k=10):
    """TÃ¬m kiáº¿m trong ChromaDB sá»­ dá»¥ng Bi-Encoder"""
    query_vector = bi_encoder.encode([query]).tolist()
    results = collection.query(query_embeddings=query_vector, n_results=top_k)

    if not results["ids"]:
        return []
    
    return [(results["metadatas"][0][i]["Question"], results["metadatas"][0][i]["Answer"]) for i in range(len(results["ids"][0]))]


# ğŸ” TÃ¬m kiáº¿m trong BM25
def search_bm25(query, top_k=10):
    """TÃ¬m kiáº¿m trong BM25 dá»±a vÃ o Combined (Question + Answer)"""
    if bm25_combined is None:
        return []

    tokenized_query = query.split()
    scores = bm25_combined.get_scores(tokenized_query)
    top_n = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:top_k]
    
    return [(doc_map[i]["Question"], doc_map[i]["Answer"]) for i, _ in top_n]

# ğŸ”¥ Káº¿t há»£p BM25 + ChromaDB vÃ  rerank vá»›i Cross-Encoder
def hybrid_search(query, top_k1=10, top_k2=10, top_m=5):
    """TÃ¬m kiáº¿m vá»›i BM25 + ChromaDB vÃ  rerank vá»›i Cross-Encoder"""
    print(f"ğŸ” Query: {query}")

    # ğŸ”¹ BÆ°á»›c 1: Láº¥y káº¿t quáº£ tá»« BM25 vÃ  ChromaDB
    bm25_results = search_bm25(query, top_k1)
    chroma_results = search_chromadb(query, top_k2)

    # ğŸ”¹ BÆ°á»›c 2: Há»£p nháº¥t káº¿t quáº£, loáº¡i bá» trÃ¹ng láº·p
    combined_results = bm25_results + chroma_results
    combined_results = remove_semantic_duplicates(combined_results)

    if not combined_results:
        return ["âŒ KhÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i phÃ¹ há»£p."]

    # ğŸ”¹ BÆ°á»›c 3: Rerank báº±ng Cross-Encoder
    cross_scores = cross_encoder.predict([(query, f"{ans[0]} {ans[1]}") for ans in combined_results])

    # ğŸ”¹ BÆ°á»›c 4: Chá»n ra top-M káº¿t quáº£ tá»‘t nháº¥t
    sorted_answers = [x for _, x in sorted(zip(cross_scores, combined_results), reverse=True)]
    final_answers = sorted_answers[:top_m]

    return final_answers


print("âœ… Há»‡ thá»‘ng tÃ¬m kiáº¿m (ChromaDB + BM25) Ä‘Ã£ sáºµn sÃ ng!")
